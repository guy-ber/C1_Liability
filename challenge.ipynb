{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI & Law - Challenge 1 - Liability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case-Study\n",
    "\n",
    "<div dir=\"auto\">\n",
    "ביום 1.1.2020 בשעה 19:47, מספר שניות לאחר שהשתלב בצומת, פגע רכב אוטונומי מתוצרת \"Play\" (יצרנית רכבים המפתחת ומשווקת רכבים אוטונומיים לצד רכבים קלאסיים) בהולך הרגל רוג'ר. \n",
    "במושב הנהג ישב בראד, אשר בעת התאונה היה עסוק במכשיר הטלפון שלו. מספר דקות לאחר התאונה, בעת חקירתו על ידי שוטרת תנועה, העיד בראד שהוא ראה את רוג'ר מתהלך באמצע הכביש אבל הניח שהרכב יעצור ולא יפגע בו. \n",
    "רוג'ר נזקק לטיפול רפואי יקר, והוא מעוניין לתבוע (במדינת פלורידה) בשל הנזקים שנגרמו לו. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite Object Detection & mAP\n",
    "\n",
    "1. [Intro to object localization and detection video](https://youtu.be/GSwYGkTfOKk) (12 minutes) by Andrew Ng from deeplearning.ai\n",
    "2. [Metrics for object detection](https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/README.md) - from \"Important definitions\" until \"How to use this project\" (without!)\n",
    "3. [mAP](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173) - mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment and Run Once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained model\n",
    "\n",
    "!mkdir -p checkpoints\n",
    "\n",
    "!wget --load-cookies /tmp/cookies.txt \\\n",
    "\"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GZ4S2o5_ICtG3Ffl1X3eCD2KbK5k3Yxt' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1GZ4S2o5_ICtG3Ffl1X3eCD2KbK5k3Yxt\" \\\n",
    "-O ./checkpoints/yolov3_ckpt_99.pth && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Sign Detection\n",
    "\n",
    "**Model:** YOLOv3 on PyTourch implemented by [Erik Linder-Norén](https://github.com/eriklindernoren/PyTorch-YOLOv3), GPL-3.0\n",
    "\n",
    "**Dataset:** [Traffic Signs Dataset in YOLO format\n",
    "](https://www.kaggle.com/valentynsichkar/traffic-signs-dataset-in-yolo-format) by [Valentyn Sichkar\n",
    "](https://valentynsichkar.name/) (Originally: [GTSDB](http://benchmark.ini.rub.de/?section=gtsdb) (German Traffic Sign Detection Benchmark) by INI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from challenge import (load_model, load_class_names,\n",
    "                       draw_detections,\n",
    "                       draw_multiple_detections,\n",
    "                       draw_evaluation,\n",
    "                       detect_multi,\n",
    "                       apply_augmentation, evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_def = 'config/yolov3-custom.cfg'\n",
    "data_config = 'config/custom.data'\n",
    "weights_path = 'checkpoints/yolov3_ckpt_99.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(weights_path, model_def, data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the traffic signs are divided into four groups, which are the lables that the model predcit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = load_class_names(data_config)\n",
    "\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find additional details about how each traffic signn is assigned to a label in [the dataset page in Kaggle](https://www.kaggle.com/valentynsichkar/traffic-signs-dataset-in-yolo-format) (Search for \"Traffic Sins in this Dataset are grouped into four categories\" section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image from the accident\n",
    "<small>Source: Google Street View</small>\n",
    "\n",
    "`./data/custom/accident/accident-day.png`\n",
    "![](./data/custom/accident/accident-day.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to classify the two images using the Traffic Sign Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function detect_multi with the argument `path_type='folder'`\n",
    "# iterates over all the images in the `path` argument,\n",
    "# it returns a list of paths as the first returned value\n",
    "# and a list of images as 4-dim numpy array (NWHC) as the second returned value\n",
    "accident_img_paths, accident_img_detections = detect_multi(model, class_names,\n",
    "                                                           path= './data/custom/accident', path_type='folder')\n",
    "\n",
    "# Draw the detections one after another\n",
    "for p, d in zip(accident_img_paths, accident_img_detections):\n",
    "    draw_detections(p, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Datasets\n",
    "\n",
    "The files `./data/custom/train.txt` and `./data/custom/valid.txt` holds the paths of the training and validation dataset imgages, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ./data/custom/train.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ./data/custom/valid.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evalauate the preformance of the model on the training dataset (remove the `report` argument to avoid printing). You can chang the path to the validataion dataset as well (`./data/custom/train.txt` → `./data/custom/valid.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "precision, recall, AP, f1, ap_class, mAP, evaluations_df = evaluate(\n",
    "    model,\n",
    "    './data/custom/train.txt',\n",
    "    class_names,\n",
    "    label_folder_path='data/custom/labels',\n",
    "    report=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two important variables:\n",
    "1. `mAP`  holds the mean Average Precision\n",
    "2. `evaluations_df` holds a pandas dataframe of the evaluation of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `evaluations_df` there are eight columns:\n",
    "1. `path` - The path to the image\n",
    "2. `tp` - For each prediction of the model, whehter it is true positive (1) or false positive (0)\n",
    "3. `pred_confs` - The confidence score of the model to each of its prediction, in the same order of `tp`\n",
    "4. `pred_labels` - The label that the model predited, in the same order of `tp`\n",
    "5. `true_labels` - The true labels\n",
    "6. `precision` - Precision of all the predictions\n",
    "7. `recall` - Recall over all the predictions\n",
    "8. `detections` - Bounding boxes of the predictions (primarly for internal use of some functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `draw_multiple_detections` we can draw evaluations (we use the method `sample` to get random images to explore, so you can run the next cell multiple times and get new images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_multiple_detections(evaluations_df.sample(4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you have also the text file `./data/custom/accident.txt` that contains the path of the two accident images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./data/custom/accident.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Augmentations\n",
    "\n",
    "To understand the impact of different augmentations on the model preformance, we can use the function `apply_augmentation` which create a copy of all the images by a paths from a text file (e.g., `./data/custom/valid.txt`), a augmentation function (e.g., `am.add_rain`) and parameters for each augmentaton (e.g., `rain_type='dizzle`, `drop_width=3`). The function returns the text file that contains all the paths to the new generated images.\n",
    "\n",
    "See the complete documentation of the augmentation functions [**here**](https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library/blob/master/README.md). You don't need to import anything new, as all the functionality is in the already existing package `augmentation`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import augmentation.automold as am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_rain_path = apply_augmentation('./data/custom/valid.txt',\n",
    "                                   am.add_rain,\n",
    "                                   rain_type='heavy',\n",
    "                                   drop_width=1)\n",
    "\n",
    "_, _, _, _, _, mAP,  df = evaluate(\n",
    "    model,\n",
    "    aug_rain_path,\n",
    "    class_names,\n",
    "    label_folder_path='data/custom/labels',\n",
    "    progress=iter,\n",
    "    report=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "\n",
    "aug_gravel_path = apply_augmentation('./data/custom/valid.txt',\n",
    "                                   am.add_gravel,\n",
    "                                   no_of_patches=45)\n",
    "\n",
    "_, _, _, _, _, mAP,  df = evaluate(\n",
    "    model,\n",
    "    aug_gravel_path,\n",
    "    class_names,\n",
    "    label_folder_path='data/custom/labels',\n",
    "    progress=iter,\n",
    "    report=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "\n",
    "aug_rain_gravel_path = apply_augmentation(aug_rain_path,\n",
    "                                   am.add_gravel,\n",
    "                                   no_of_patches=45)\n",
    "\n",
    "_, _, _, _, _, mAP,  df = evaluate(\n",
    "    model,\n",
    "    aug_rain_gravel_path,\n",
    "    class_names,\n",
    "    label_folder_path='data/custom/labels',\n",
    "    progress=iter,\n",
    "    report=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your analysis, focus on `am.add_rain` and `am.add_gravel`. We think about the latter as dirt on the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "\n",
    "_, _, _, _, _, _, evaluations_df = evaluate(\n",
    "    model,\n",
    "    './data/custom/valid.txt',\n",
    "    class_names,\n",
    "    label_folder_path='data/custom/labels',\n",
    "    report=False\n",
    ")\n",
    "\n",
    "aug_paths_files = {}\n",
    "\n",
    "for rain_type in ['dizzle', 'heavy']:\n",
    "    for drop_width in trange(2, 3):\n",
    "        aug_paths_files[f'rain-{rain_type}-{drop_width}'] = apply_augmentation('./data/custom/valid.txt',\n",
    "                                                                   am.add_rain,\n",
    "                                                                   rain_type=rain_type,\n",
    "                                                                   drop_width=drop_width)\n",
    "\n",
    "for aug_tag, prev_aug_path in aug_paths_files.copy().items():\n",
    "    for no_of_patches in trange(45, 55, 10):\n",
    "        aug_paths_files[f'{aug_tag}_gravel-{no_of_patches}'] = apply_augmentation(prev_aug_path,\n",
    "                                                                   am.add_gravel,\n",
    "                                                                   no_of_patches=no_of_patches)\n",
    "    \n",
    "for no_of_patches in trange(45, 55, 10):\n",
    "    aug_paths_files[f'gravel-{no_of_patches}'] = apply_augmentation('./data/custom/valid.txt',\n",
    "                                                               am.add_gravel,\n",
    "                                                               no_of_patches=no_of_patches)\n",
    "\n",
    "\n",
    "mAPs = {}\n",
    "evaluations_dfs = {}\n",
    "\n",
    "for aug, path in aug_paths_files.items():\n",
    "    print(f'### {aug} ###')\n",
    "    _, _, _, _, _, mAP,  df = evaluate(\n",
    "        model,\n",
    "        path,\n",
    "        class_names,\n",
    "        label_folder_path='data/custom/labels',\n",
    "        progress=iter,\n",
    "        report=False\n",
    "    )\n",
    "    \n",
    "    mAPs[aug] = mAP\n",
    "    evaluations_dfs[aug] = df\n",
    "    \n",
    "mAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "@interact(augmentation=list(tuple(evaluations_dfs.keys())),\n",
    "          index=(0, len(evaluations_df)-1),\n",
    "          mAPs=fixed(mAPs))\n",
    "def explore_augmentation(index, augmentation, mAPs):\n",
    "    _, axes = plt.subplots(1, 2, figsize=(15, 15))\n",
    "    \n",
    "    draw_evaluation(evaluations_dfs[augmentation].iloc[index], f'Augmented (mAP={mAPs[augmentation]}\\n',\n",
    "                    ax=axes[0]);\n",
    "    draw_evaluation(evaluations_df.iloc[index], 'Original\\n', ax=axes[1]);\n",
    "\n",
    "    axes[0].axis('off');\n",
    "    axes[1].axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "\n",
    "# Generate accident day image\n",
    "# NOTE: Sometimes the classifer managed to work wall on the image with heavy rain 2,\n",
    "#       so multiple runs are required for a FN\n",
    "\n",
    "# Remove this to generate the accident images\n",
    "raise RuntimeError\n",
    "\n",
    "accident_rain_path = apply_augmentation('./data/custom/accident.txt',\n",
    "                       am.add_rain,\n",
    "                       rain_type='heavy',\n",
    "                       drop_width=2)\n",
    "\n",
    "accident_gravel_path = apply_augmentation('./data/custom/accident.txt',\n",
    "                       am.add_gravel,\n",
    "                       no_of_patches=45)\n",
    "\n",
    "\n",
    "accident_rain_gravel_path = apply_augmentation(accident_rain_path,\n",
    "                                               am.add_gravel,\n",
    "                                               no_of_patches=45)\n",
    "\n",
    "accident_img_paths, accident_img_detections = detect_multi(model, class_names,\n",
    "                                                           'data/custom/accident--aug-add_rain_heavy-2', 'folder',\n",
    "                                                            progress=iter)\n",
    "\n",
    "for p, d in zip(accident_img_paths, accident_img_detections):\n",
    "    draw_detections(p, d)\n",
    "\n",
    "accident_img_paths, accident_img_detections = detect_multi(model, class_names,\n",
    "                                                           'data/custom/accident--aug-add_gravel_45', 'folder',\n",
    "                                                            progress=iter)\n",
    "\n",
    "for p, d in zip(accident_img_paths, accident_img_detections):\n",
    "    draw_detections(p, d)\n",
    "    \n",
    "accident_img_paths, accident_img_detections = detect_multi(model, class_names, './data/custom/accident--aug-add_rain_heavy-2--aug-add_gravel_45',\n",
    "                                         path_type='folder', progress=iter)\n",
    "\n",
    "for p, d in zip(accident_img_paths, accident_img_detections):\n",
    "    draw_detections(p, d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
